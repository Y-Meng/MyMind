{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.Jupyter常用快捷键\n",
    "\n",
    "* **Shift + Enter** : 执行当前cell并选中下个cell\n",
    "* **Shift + J 或 Shift + Down** : 选择下一个cell\n",
    "* **Shift + K 或 Shift + Up** : 选中上一个cell\n",
    "* **Shift + M** : 合并选中cell\n",
    "* **Ctrl + Enter** : 执行下一个cell\n",
    "\n",
    "* **Enter** ： 编辑模式\n",
    "\n",
    "* **Esc** : 命令模式\n",
    "* **Esc a** : 在上方新建cell\n",
    "* **Esc b** : 在下方新建cell\n",
    "* **Esc m** : 切换到markdown\n",
    "* **Esc y** : 切换到code\n",
    "* **Esc l** : 显示行数\n",
    "* **Esc d d** : 删除选中的cell\n",
    "* **Esc 0** : 收起、打开output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.python正则库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, World!\n"
     ]
    }
   ],
   "source": [
    "# encoding: UTF-8\n",
    "# py3默认编码utf-8，无需声明\n",
    "import re\n",
    "pattern = re.compile(r'hello.*\\!')\n",
    "match = pattern.match('hello, World! how are you?')\n",
    "if match:\n",
    "    print(match.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.jieba中文处理\n",
    "\n",
    "代码对 Python 2/3 均兼容\n",
    "\n",
    "安装：\n",
    "* 全自动安装：easy_install jieba 或者pip install jieba / pip3 install jieba\n",
    "* 半自动安装：先下载 http://pypi.python.org/pypi/jieba/ ，解压后运行 python setup.py install\n",
    "* 手动安装：将 jieba 目录放置于当前目录或者 site-packages 目录\n",
    "\n",
    "特点：\n",
    "* 支持三种分词模式：\n",
    " * 精确模式，试图将句子最精确地切开，适合文本分析；\n",
    " * 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；\n",
    " * 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。\n",
    "* 支持繁体分词\n",
    "* 支持自定义词典\n",
    "\n",
    "\n",
    "## 2.1.基本分词函数与用法\n",
    "\n",
    "### jieba.cut() & jieba.lcut() 接受三个参数：\n",
    "\n",
    "* 需要分词的字符串\n",
    "* cut_all 是否采用全模式\n",
    "* HMM 是否使用HMM（隐马）模型\n",
    "\n",
    "### jieba.cut_for_search() & jieba.lcut_for_search() 方法接受两个参数：\n",
    "\n",
    "* 需要分词的字符串\n",
    "* 是否使用HMM模型\n",
    "\n",
    "> * 该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细\n",
    "> * 待分词的字符串可以是 unicode 或 UTF-8 字符串、GBK 字符串。注意：不建议直接输入 GBK 字符串，可能无法预料地错误解码成 UTF-8\n",
    "> * jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)，或者用\n",
    "jieba.lcut 以及 jieba.lcut_for_search 直接返回 list\n",
    "\n",
    "### jieba.Tokenizer(dictionary=DEFAULT_DICT) 新建自定义分词器\n",
    "> 可用于同时使用不同词典。jieba.dt 为默认分词器，所有全局分词相关函数都是该分词器的映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全模式：我/在/学习/自然/自然语言/语言/处理\n",
      "默认精准模式：我/在/学习/自然语言/处理\n",
      "搜索引擎模式：我/在/学习/自然/语言/自然语言/处理\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "# 全模式\n",
    "seg_list = jieba.cut(\"我在学习自然语言处理\", cut_all=True)\n",
    "print(\"全模式：\" + \"/\".join(seg_list))\n",
    "\n",
    "# 精准模式(默认)\n",
    "# set_list = jieba.cut(\"我在学习自然语言处理\")\n",
    "seg_list = jieba.cut(\"我在学习自然语言处理\", cut_all=False)\n",
    "print(\"默认精准模式：\" + \"/\".join(seg_list))\n",
    "\n",
    "# 搜索引擎模式\n",
    "seg_list = jieba.cut_for_search(\"我在学习自然语言处理\")\n",
    "print(\"搜索引擎模式：\" + \"/\".join(seg_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.用户自定义词典\n",
    "\n",
    "开发者可以指定自己自定义的词典，以便包含 jieba 词库里没有的词。虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率。\n",
    "自定义添加词典有两种方式：\n",
    "\n",
    "1. 用jieba.load_userdict(file_name)加载用户词典，file_name为自定义词典路径；\n",
    "\n",
    "词典格式如下，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。\n",
    "```\n",
    "创新办 3 i\n",
    "云计算 5\n",
    "凱特琳 nz\n",
    "台中\n",
    "```\n",
    "\n",
    "2. 少量词汇可以自己手动加载\n",
    " * add_word(word, freq=None, tag=None) 和 del_word(word)动态添加和删除\n",
    " * suggest_freq(segment, tune=True)可调节单个词语的词频，使其能（或不能）被分出来\n",
    " * 注意：自动计算的词频在使用 HMM 新词发现功能时可能无效。动态添加词语后分词，需要讲HMM关闭。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果/放到/post/中/将/出错\n",
      "如果/放到/post/中将/出错\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "print(\"/\".join(jieba.cut(\"如果放到post中将出错\", HMM=False)))\n",
    "\n",
    "# 调整词频使“中将”不被切开\n",
    "jieba.suggest_freq(\"中将\", True)\n",
    "print(\"/\".join(jieba.cut(\"如果放到post中将出错\", HMM=False)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.关键词提取\n",
    "\n",
    "### 基于TF-IDF算法的关键词抽取\n",
    "\n",
    "* jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS={})\n",
    " * sentencesentence 为待提取的文本\n",
    " * topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20\n",
    " * withWeight 为是否一并返回关键词权重值，默认值为 False\n",
    " * allowPOS 仅包括指定词性的词，默认值为空，即不筛选\n",
    "\n",
    "* jieba.analyse.TFIDF(idf_path=None) 新建 TFIDF 实例，idf_path 为 IDF 频率文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行者,八戒,师父,三藏,大圣,唐僧,沙僧,菩萨,妖精,和尚,那怪,甚么,那里,长老,呆子,怎么,不知,徒弟,老孙,悟空\n",
      "行者_0.13416769104125637\n",
      "八戒_0.06138749884413054\n",
      "师父_0.055801508802814426\n",
      "三藏_0.04951909810584418\n",
      "大圣_0.03438635079555997\n",
      "唐僧_0.03093405521307161\n",
      "沙僧_0.028649007279639396\n",
      "菩萨_0.028405867765903325\n",
      "妖精_0.025323223636966256\n",
      "和尚_0.02221375423601498\n",
      "那怪_0.020860372519413882\n",
      "甚么_0.020631252946982325\n",
      "那里_0.019782992142335202\n",
      "长老_0.018835980952815547\n",
      "呆子_0.017935812786738954\n",
      "怎么_0.017029691574655566\n",
      "不知_0.01675812196592144\n",
      "徒弟_0.016753840674141294\n",
      "老孙_0.016216444824348537\n",
      "悟空_0.01608917368516298\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "content = open(\"西游记.txt\", 'rb').read()\n",
    "\n",
    "# 关键词抽取\n",
    "tags = jieba.analyse.extract_tags(content, topK=20)\n",
    "print(\",\".join(tags))\n",
    "\n",
    "# 关键词一并返回关键词权重值示例\n",
    "tags = jieba.analyse.extract_tags(content, topK=20, withWeight=True)\n",
    "for tag in tags:\n",
    "    print(\"{}_{}\".format(tag[0], tag[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关键词提取所使用逆向文件频率（IDF）文本语料库可以切换成自定义语料库的路径\n",
    "\n",
    "* 用法： jieba.analyse.set_idf_path(file_name) # file_name为自定义语料库的路径\n",
    "* 自定义语料库示例：https://github.com/fxsjy/jieba/blob/master/extra_dict/idf.txt.big\n",
    "* 用法示例：https://github.com/fxsjy/jieba/blob/master/test/extract_tags_idfpath.py\n",
    "\n",
    "关键词提取所使用停止词（Stop Words）文本语料库可以切换成自定义语料库的路径\n",
    "\n",
    "* 用法： jieba.analyse.set_stop_words(file_name) # file_name为自定义语料库的路径\n",
    "* 自定义语料库示例：https://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt\n",
    "* 用法示例：https://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于TextRank算法的关键词抽取\n",
    "\n",
    "基于PageRank算法，数据量越大效果越好。\n",
    "\n",
    "* jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 直接使用，接口相同，注意默认过滤词性。\n",
    "* jieba.analyse.TextRank() 新建自定义 TextRank 实例\n",
    "\n",
    "算法论文：[TextRank: Bringing Order into Texts](http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n",
    "\n",
    "基本思想：\n",
    "1. 将待抽取关键词的文本进行分词\n",
    "2. 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图\n",
    "3. 计算图中节点的PageRank，注意是无向带权图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行者_1.0\n",
      "师父_0.4468256730411676\n",
      "八戒_0.40251824218027515\n",
      "三藏_0.37841352093521363\n",
      "大圣_0.2665149124004022\n",
      "菩萨_0.23124570809321954\n",
      "不知_0.20126241891518512\n",
      "只见_0.1980733274447043\n",
      "妖精_0.18546445974373882\n",
      "长老_0.15033785015367465\n",
      "国王_0.14145894256193234\n",
      "呆子_0.10492607679999778\n",
      "徒弟_0.1015174633383265\n",
      "悟空_0.09583793442461931\n",
      "不见_0.08953723908347787\n",
      "不得_0.08825993684157182\n",
      "小妖_0.08814565706803631\n",
      "不能_0.0876208317954937\n",
      "出来_0.08516932481414757\n",
      "铁棒_0.0735751184496777\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.posseg\n",
    "import jieba.analyse\n",
    "\n",
    "content = open(\"西游记.txt\", \"rb\").read()\n",
    "\n",
    "tags = jieba.analyse.textrank(content, withWeight=True)\n",
    "for tag in tags:\n",
    "    print(\"{}_{}\".format(tag[0], tag[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.词性标注\n",
    "\n",
    "* jieba.posseg.POSTokenizer(tokenizer=None) 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。\n",
    "* 标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "爱 v\n",
      "北京 ns\n",
      "天安门 ns\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"我爱北京天安门\")\n",
    "for word,flag in words:\n",
    "    print(\"%s %s\" %(word, flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.并行分词\n",
    "\n",
    "### 原理：\n",
    "\n",
    "将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升\n",
    "\n",
    "基于 python 自带的 multiprocessing 模块，目前暂不支持 Windows。\n",
    "\n",
    "### 用法：\n",
    "jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数\n",
    "jieba.disable_parallel() # 关闭并行分词模式\n",
    "\n",
    "注意：并行分词仅支持默认分词器 jieba.dt 和 jieba.posseg.dt。\n",
    "\n",
    "## 2.6.Tokenize：返回词语在原文起止位置\n",
    "\n",
    "注意，输入参数只接受 unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 我\t\t start: 0 \t\t end:1\n",
      "word 爱\t\t start: 1 \t\t end:2\n",
      "word 北京\t\t start: 2 \t\t end:4\n",
      "word 天安门\t\t start: 4 \t\t end:7\n",
      "word 永和\t\t start: 0 \t\t end:2\n",
      "word 服装\t\t start: 2 \t\t end:4\n",
      "word 饰品\t\t start: 4 \t\t end:6\n",
      "word 有限\t\t start: 6 \t\t end:8\n",
      "word 公司\t\t start: 8 \t\t end:10\n",
      "word 有限公司\t\t start: 6 \t\t end:10\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "# 默认模式\n",
    "result = jieba.tokenize(u\"我爱北京天安门\")\n",
    "for tk in result:\n",
    "    print(\"word %s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))\n",
    "\n",
    "# 搜索模式\n",
    "result = jieba.tokenize(u'永和服装饰品有限公司', mode='search')\n",
    "for tk in result:\n",
    "    print(\"word %s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7.命令行操作\n",
    "\n",
    "使用示例：python -m jieba news.txt > cut_result.txt\n",
    "\n",
    "命令行选项（翻译）：\n",
    "\n",
    "使用: python -m jieba [options] filename\n",
    "\n",
    "结巴命令行界面。\n",
    "\n",
    "固定参数:\n",
    "  filename              输入文件\n",
    "\n",
    "可选参数:\n",
    "  -h, --help            显示此帮助信息并退出\n",
    "  -d [DELIM], --delimiter [DELIM]\n",
    "                        使用 DELIM 分隔词语，而不是用默认的' / '。\n",
    "                        若不指定 DELIM，则使用一个空格分隔。\n",
    "  -p [DELIM], --pos [DELIM]\n",
    "                        启用词性标注；如果指定 DELIM，词语和词性之间\n",
    "                        用它分隔，否则用 _ 分隔\n",
    "  -D DICT, --dict DICT  使用 DICT 代替默认词典\n",
    "  -u USER_DICT, --user-dict USER_DICT\n",
    "                        使用 USER_DICT 作为附加词典，与默认词典或自定义词典配合使用\n",
    "  -a, --cut-all         全模式分词（不支持词性标注）\n",
    "  -n, --no-hmm          不使用隐含马尔可夫模型\n",
    "  -q, --quiet           不输出载入信息到 STDERR\n",
    "  -V, --version         显示版本信息并退出\n",
    "\n",
    "如果没有指定文件名，则使用标准输入"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
