{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.LDA主题模型理论\n",
    "\n",
    "LDA latent Dirichlet Allocation\n",
    "\n",
    "是一种无监督贝叶斯模型\n",
    "是一种主题模型，它可以将文档集合中每篇文档的主题按照概率分布的形式给出。\n",
    "同时它是一种无监督学习方法，在训练时不需要手工标注训练集，需要的仅仅是文档集以及指定主题的数量k即可。\n",
    "**此外LDA的另一个优点则是，对于每一个主题均可找出一些词语来描述它（主题自描述）**。\n",
    "\n",
    "是一种典型的词袋模型，即它认为一个文档是由一组词构成的一个集合，词与词之间没有顺序以及先后顺序的关系。一篇文档可以包含多个主题，文档中每个词都由其中的一个主题生成。\n",
    "\n",
    "## 1.1.直观版\n",
    "\n",
    "### 举例\n",
    "\n",
    "假设某企业招聘工程师\n",
    "收到一堆简历\n",
    "希望直接通过简历筛选牛人\n",
    "\n",
    "简历通常包含这些个人特征：百度实习、top2毕业。。。\n",
    "\n",
    "特征--简历--优秀／不优秀\n",
    "\n",
    "问题：你并不知道优秀的具有哪些特征，不优秀的具备哪些特征\n",
    "\n",
    "1. 开始猜\n",
    "2. 证实猜想\n",
    "3. 反复1-2\n",
    "4. 特征分类\n",
    "\n",
    "### 理论\n",
    "\n",
    "贝叶斯模型：\n",
    "\n",
    "$P(Y,X) = P(Y)P(X|Y) = P(X)P(Y|X)$\n",
    "\n",
    "$P(Y|X) = \\frac{P(Y,X)}{P(X)}  = \\frac{P(Y)P(X|Y)}{P(X)} = \\frac{P(X)P(Y|X)}{P(X)}$\n",
    "\n",
    "例如：$P(堵车|车祸) = \\frac{P(堵车)P(车祸|堵车)}{P(车祸)}$\n",
    "\n",
    "#### 贝叶斯学派：先验，后验，似然\n",
    "\n",
    "* 先验概率：根据经验得到的结果的概率（已知结果） P(Y)\n",
    "* 后验概率：在知道原因的情况下，求结果发生的概率（执因求果）P(Y|X)\n",
    "* 似然概率：知道结果的情况下，求最可能导致结果发生的原因（知果求因）P(X|Y)\n",
    "\n",
    "\n",
    "> 贝叶斯模型的伟大之处在于在条件概率计算中实现先验后验的转换。\n",
    "\n",
    "\n",
    "\n",
    "## 1.2.标准版\n",
    "\n",
    "> $P(Y|X) = \\frac{P(Y,X)}{P(X)}  = \\frac{P(Y)P(X|Y)}{P(X)}$\n",
    "\n",
    "> $P(Y|Z) = \\frac{P(Y,Z)}{P(Z)}  = \\frac{P(Y)P(Z|Y)}{P(Z)}$\n",
    "\n",
    "> $P(Z|X) = \\frac{P(Z,X)}{P(X)}  = \\frac{P(Z)P(X|Z)}{P(X)}$\n",
    "\n",
    "\n",
    "> $P(Y|Z) * P(Z|X) = \\frac{P(Y)P(Z|Y)}{P(Z)} * \\frac{P(Z)P(X|Z)}{P(X)} =...=P(Y|X) $\n",
    "\n",
    "\n",
    "一份简历中的每个特征都是因为候选人有一定概率是好／坏的程序员，并从好／坏这个分类中以一定概率选择某些特征组成。\n",
    "\n",
    "一篇文章的每个词都是以一定概率选择了某个主题，并从这个主题中以一定的概率选择某个词语而组成。\n",
    "\n",
    "> $P(单词|文档)=P(单词|主题)*P(主题|文档)$\n",
    "\n",
    "\n",
    "### LDA生成过程\n",
    "\n",
    "对于语料库中每篇文档，LDA定义如下生成过程（generative process）:\n",
    "1. 对于每篇文档，从主题分布中抽取一个主题；\n",
    "2. 从上述被抽到的主题所对应的单词分布中抽取一个单词；\n",
    "3. 通过单词在文档中的分布（可观测）拟合验证；\n",
    "4. 重复上述过程直至便利文档中的每一个单词。\n",
    "\n",
    "具体：（w代表单词；d代表文档；t代表主题；大写代表总集合，小写代表个体）\n",
    "\n",
    "* D中每个文档看作一个单词序列<w1,w2,w3,...,wn>,wi表斯第i个单词；\n",
    " \n",
    "* D中涉及所有不同单词组成一个词汇表大集合V（vocabulary），LDA以文档集合D作为输入，希望训练出两个结果向量（设形成k个主题topic，V中共m个单词）；\n",
    "\n",
    " 1. 对D中每个文档d，对应到k个不同topic的概率 Pd<pt1,pt2,pt3,...,ptk>，其中pti表示d对应T中第i个topic的概率。计算方法：pti=nti/n,nti表示d中对应第i个topic的词的数目，n是d中所有词的总数；\n",
    " 2. 对T中每个主题t，生成不同单词的概率Pt<pw1,pw2,pw3,...,pwm>，其中pwi表示t生成V中第i个单词的概率。计算方法：pwi=Nwi/N,Nwi表示对应到t中的V中第i个单词的数目，N表示所有对应到t中单词的总数。\n",
    " \n",
    "* 通过文章中出现单词的客观概率验证修正Pd 和 Pt\n",
    "> P(w|d)=P(w|t) * P(t|d)\n",
    "\n",
    "直观的看这个公式，就是以topic作为中间层，可以通过当前的Pd和Pt给出文档d中出现单词w的概率。其中p(t|d)通过Pd计算，p(w|t)通过Pt计算；\n",
    "实际上利用Pd和Pt，我们可以为一个文档中的一个单词计算它对应任意一个topic的p(w|d)，然后根据这些结果更新这个词应该对应的topic。然后，如果这个更新改变了单词所对应的topic，就会反过来影响Pt 和 Pd。\n",
    "\n",
    "### LDA学习过程\n",
    "\n",
    "算法开始时先给Pt和Pd随机赋值；然后：\n",
    "1. 针对一个特定文档d中的第i个单词wi，如果令该单词对应的topic为tj，可以把上述公式改写为：Pj(wi|d)=P(wi|tj) * P(tj|d)\n",
    "2. 现在我们可以枚举T中的topic，得到所有的pj(wi|d)。然后可以根据这些概率值结果为d中第i个单词wi选择一个topic。最简单的想法是取令pj(wi|d)最大的tj(这里只有j是变量)\n",
    "3. 然后，如果d中单词遍历过程中的第i个单词wi在这里选择了一个与原先不同的topic，就会对Pt和Pd有影响。它们的影响优惠反过来影响上面p(w|d)的计算。对D中所有d中的所有w进行一次p(w|d)计算并重新选择topic看作一次迭代。这样进行n次循环迭代就会收敛LDA想要的结果了。\n",
    "\n",
    "\n",
    "## 1.3.公式版\n",
    "\n",
    "### 共轭分布与共轭先验\n",
    "\n",
    "后验概率（posterior probability） =》 似然函数（likelihood function）* 先验概率（prior probability）\n",
    "\n",
    "### gamma函数\n",
    "\n",
    "### 二项分布（Binomial distribution）\n",
    "\n",
    "### 多项分布\n",
    "\n",
    "### beta分布，二项分布的共轭先验分布\n",
    "\n",
    "### Dirichlet分布，beta分布在高纬度上的推广\n",
    "\n",
    "### 几个模型\n",
    "\n",
    "* Unigram model\n",
    "* mix unigram\n",
    "* PLSA模型\n",
    "* LDA 是贝叶斯版 PLSA\n",
    "\n",
    "PLSA是频率派思想，LDA是贝叶斯派思想\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.LDA实战-希拉里邮件门\n",
    "\n",
    "## 2.1.读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"../input/HillaryEmails.csv\")\n",
    "# 去除Nan值\n",
    "df = df[['Id','ExtractedBodyText']].dropna()\n",
    "\n",
    "def clean_email_text(text):\n",
    "    text = text.replace('\\n','')\n",
    "    # 把“-”两个单词分开 july-edu => july edu\n",
    "    text = re.sub(r\"-\",\" \",text)\n",
    "    # 去除日期\n",
    "    text = re.sub(r\"\\d+/\\d+/\\d+\",\"\",text)\n",
    "    # 邮件地址\n",
    "    text = re.sub(r\"[\\w]+@[\\.\\w]+\", \"\", text)\n",
    "    # 去网址\n",
    "    text = re.sub(r\"https?://[\\.\\w]+\",\"\", text)\n",
    "    pure_text = \"\"\n",
    "    for letter in text:\n",
    "        # 只留下字母和空格\n",
    "        if letter.isalpha() or letter==\" \":\n",
    "            pure_text += letter\n",
    "            \n",
    "    text = \" \".join(word for word in pure_text.split() if len(word) > 1)\n",
    "    return text\n",
    "\n",
    "docs = df['ExtractBodyText']\n",
    "docs = docs.apply(lambda s: clean_email_text(s))\n",
    "\n",
    "docs.head(1).values\n",
    "\n",
    "doclist = docs.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.LDA 模型构建\n",
    "\n",
    "使用Gensim做模型构建\n",
    "首先把刚刚整理出来的文本数据，转化为Gemsim认可的语料库形式：\n",
    "[[a,bc,d],[e,fg,h]...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import gensim\n",
    "\n",
    "# 免去安装nltk，手写停止词列表\n",
    "stoplist = ['vary','ourselves','am','']\n",
    "\n",
    "# 分词\n",
    "texts = [[word for word in doc.lower().split() if word not in stoplist] for doc in doclist]\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标记化，把每个单词用一个数字index指代，把原文本比变成一个个数组\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# 建立训练模型\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20)\n",
    "\n",
    "# 查看10号主题\n",
    "lda.print_topic(10, topn=5)\n",
    "\n",
    "# 打印所有主题\n",
    "lda.print_topics(num_topics=20, num_words=5)\n",
    "\n",
    "# 使用模型对文本（标记数组化后）分类\n",
    "bow = [1,2,3]\n",
    "lda.get_document_topics(bow)\n",
    "\n",
    "# 使用模型对单词分类\n",
    "word_id = 2\n",
    "lda.get_term_topics(word_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
